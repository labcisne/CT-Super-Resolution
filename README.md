# SUPER RESOLUTION OF LOW DOSAGE CT IMAGES: COMPARISON OF DEEP LEARNING METHODS

## Table of Contents
1. [Introduction](#introduction)
2. [Dataset Description](#dataset-description)
3. [Models Used for Super Resolution](#models-used-for-super-resolution)
4. [Project Structure](#project-structure)
5. [How to Run](#how-to-run)
6. [Evaluation Metrics](#evaluation-metrics)
7. [Contributions](#contributions)

## Introduction

This project aims to enhance low-dose computed tomography (CT) images using deep learning-based **super-resolution** methods. Super-resolution techniques allow for the reconstruction of high-quality images from low-dosage CT scans, which are typically noisy and less detailed. The project compares several state-of-the-art models for image super-resolution, using a combination of both perceptual and quantitative metrics to determine the best performance.

## Dataset Description

The dataset is structured as follows:
- **Observation CT Images**: Low-dosage, noisy images serving as input.
- **Ground Truth CT Images**: High-quality CT images used as the reference standard.
- **Super-resolved Images**: Images generated by various super-resolution models.

The images are organized in directories:
- `Dataset/observation_test_images/`: Low-dosage CT images.
- `Dataset/ground_truth_test_images/`: Full-dose CT reference images.
- `Dataset/model_X/`: Contains super-resolved images generated by each model (e.g., SRCNN, ESRGAN, etc.).

## Models Used for Super Resolution

The deep learning models used in this study include:

1. **SRCNN (Super-Resolution Convolutional Neural Network)**: An early CNN-based super-resolution model.
2. **ESRGAN (Enhanced Super-Resolution Generative Adversarial Network)**: Known for generating sharp, realistic images.
3. **SwinIR (Swin Transformer for Image Restoration)**: A transformer-based approach focusing on both local and global features.
4. **HAT (Hybrid Attention Transformer)**: It combines both channel attention and window-based self-attention schemes.
5. **DAT (Dual Aggregation Transformer)**: DAT aggregates features across spatial and channel dimensions, in the inter-block and intra-block dual manner.

## Project Structure

- `Dataset/`: Contains observation, ground truth, and model-generated images.

## How to Run

1. **Prepare the Dataset**: Make sure the `Dataset/` folder is populated with observation, ground truth, and model output images as outlined above.

2. **Execute the Cells**:

   - **Image Loading**: Loads observation, ground truth, and model-generated images.
   - **Evaluation**: Metrics like PSNR, SSIM, LPIPS, NIQE, NRQM, and PI are calculated for each model.
   - **Visualization**: Displays comparison plots between the observation image, super-resolved images, and ground truth.

## Evaluation Metrics

The following metrics are used to evaluate the performance of each super-resolution model:

1. **PSNR (Peak Signal-to-Noise Ratio)**: Measures the reconstruction quality of the super-resolved images compared to the ground truth. Higher values indicate better quality.
   
2. **SSIM (Structural Similarity Index)**: Assesses the perceptual similarity between the super-resolved and ground truth images, focusing on luminance, contrast, and structure. Higher values indicate higher similarity.
   
3. **LPIPS (Learned Perceptual Image Patch Similarity)**: A perceptual metric that compares deep features between two images. Lower values indicate higher perceptual similarity.

4. **NIQE (Natural Image Quality Evaluator)**: A no-reference image quality metric. Lower values indicate better naturalness and less perceptual distortion.

5. **NRQM (Non-Reference Quality Metric)**: Another no-reference metric used to evaluate the quality of images without needing a ground truth. Higher values indicate better image quality.

6. **PI (Perceptual Index)**: Combines NIQE and NRQM into a single perceptual quality index. Lower values represent better perceived image quality.

## Visualization and Comparison

A key feature of the project is the visual comparison of model outputs with the observation and ground truth images. The comparison is displayed in a grid that includes:

- **Observation Image**: The input low-dose CT image.
- **Super-Resolved Images**: Outputs from the models (SRCNN, ESRGAN, SwinIR, HAT, DAT).
- **Ground Truth Image**: The high-quality, full-dose CT image.

All comparison plots are saved as high-resolution PNG files and one is displayed interactively for each test case.

## Contributions

Feel free to fork this project and submit pull requests for any improvements, additional models, or bug fixes. Contributions, issues, and feature requests are welcome.

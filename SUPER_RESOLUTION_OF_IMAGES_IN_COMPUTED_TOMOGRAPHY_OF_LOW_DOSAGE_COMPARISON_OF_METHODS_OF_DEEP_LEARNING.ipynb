{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SruHzdP7MzxV"
      },
      "source": [
        "# Cornerstone Project - Eric Rodrigues de Carvalho\n",
        "\n",
        "This notebook is a series of codes that serves as one of the primary tools to execute my cornerstone project as an undergraduated student called \"SUPER-RESOLUÇÃO DE IMAGENS EM\n",
        "TOMOGRAFIA COMPUTADORIZADA DE BAIXA\n",
        "DOSAGEM: COMPARAÇÃO DE MÉTODOS DE\n",
        "APRENDIZADO PROFUNDO\" (SUPER-RESOLUTION OF IMAGES IN\n",
        "COMPUTED TOMOGRAPHY OF LOW\n",
        "DOSAGE: COMPARISON OF METHODS OF\n",
        "DEEP LEARNING).\n",
        "\n",
        "The main goals of this notebook are:\n",
        "\n",
        "1. Convert files from HDF5 (the source file) to actual PNG images.\n",
        "2. Convert sinogram representation to actual and human-readable CT-scanned images.\n",
        "3. Convert SRCNN model to ONNX-based in order to run in chaiNNer application.\n",
        "4. After using chaiNNer as specified in the thesis, execute metrics in a subset of images to evaluate DL models performance in both no-reference and full-reference metrics and also its runtime.\n",
        "5. Plot examples for visual comparision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO78Uom8Pmua"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Un-comment if you wish to use Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv2Smk-bOzcK"
      },
      "source": [
        "## HDF to BMP and Sinogram to human-readable CT\n",
        "\n",
        "Download the zip containing the datasets at https://zenodo.org/records/3384092. For this cornerstone project, only ground_truth_test.zip and observation_test.zip are needed. After unzipping in the folder of preference, set your dataset_path and the hdf5_folders based on your own desired folders' structure\n",
        "\n",
        "This script processes two datasets (ground truth and observation) from HDF5 files, typically used in medical imaging or computed tomography reconstruction scenarios. It converts these data into BMP image format and saves them into pre-defined directories.\n",
        "\n",
        "Key steps in the code:\n",
        "\n",
        "Directory Creation: The script ensures that the directories for storing the images (both ground truth and observation) exist. If they don't, it creates them using the create_dir function.\n",
        "\n",
        "Ground Truth and Observation Processing: It reads HDF5 files containing arrays of image data. The ground truth data is directly saved as images after rotating them. The observation data, on the other hand, undergoes more complex processing: it performs a reconstruction using the iradon function (Filtered Back Projection) and crops the resulting image to a standard size before saving.\n",
        "\n",
        "Error Handling: The code handles potential errors in reading files. If an error occurs while processing observation data, the corresponding ground truth image is deleted to maintain data consistency.\n",
        "\n",
        "FBP Reconstruction: The observation data is reconstructed using the iradon function from the skimage.transform library. This method applies a transformation called Filtered Back Projection, commonly used in medical imaging to reconstruct cross-sectional images from projection data (as in CT scans).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ur1Eb40Q-VR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This script processes medical imaging data stored in HDF5 format. Specifically, it reads the\n",
        "ground truth and observation test datasets, processes them into images, and saves them in BMP format.\n",
        "\n",
        "The script does the following:\n",
        "1. Creates directories for saving ground truth and observation images if they don't exist.\n",
        "2. Reads ground truth and observation HDF5 files.\n",
        "3. For each file:\n",
        "   - Ground truth images are rotated and saved directly.\n",
        "   - Observation data is used to reconstruct images using the Filtered Back Projection (FBP) technique,\n",
        "     crop the result, and save the reconstructed images.\n",
        "4. If an error occurs in reading the observation file, the corresponding ground truth image is deleted to\n",
        "   ensure data consistency.\n",
        "\n",
        "Dependencies:\n",
        "- h5py: For reading HDF5 files.\n",
        "- numpy: For numerical operations, including image manipulation.\n",
        "- matplotlib: For saving images in BMP format.\n",
        "- skimage: For performing the Filtered Back Projection (FBP) reconstruction.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import iradon\n",
        "\n",
        "# Path to the dataset folder containing HDF5 files\n",
        "dataset_path = 'your/path/here'\n",
        "\n",
        "# List of folder names within the dataset containing various datasets (train, test, validation)\n",
        "hdf5_folders = [\n",
        "    'ground_truth_train',\n",
        "    'ground_truth_test',\n",
        "    'ground_truth_validation',\n",
        "    'observation_train',\n",
        "    'observation_test',\n",
        "    'observation_validation',\n",
        "]\n",
        "\n",
        "# Function to create directories if they do not exist\n",
        "\n",
        "\n",
        "def create_dir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "# Process ground truth test data by saving images in a specific directory\n",
        "ground_truth_test_images_dir = os.path.join(\n",
        "    dataset_path, 'ground_truth_test_images')\n",
        "create_dir(ground_truth_test_images_dir)\n",
        "\n",
        "# Process observation test data by saving images in a specific directory\n",
        "observation_test_images_dir = os.path.join(\n",
        "    dataset_path, 'observation_test_images')\n",
        "create_dir(observation_test_images_dir)\n",
        "\n",
        "# Get a sorted list of ground truth and observation HDF5 files\n",
        "ground_truth_files = sorted(os.listdir(\n",
        "    os.path.join(dataset_path, hdf5_folders[1])))\n",
        "observation_files = sorted(os.listdir(\n",
        "    os.path.join(dataset_path, hdf5_folders[4])))\n",
        "\n",
        "# Initialize counters for naming the saved image files\n",
        "ground_truth_counter = 0\n",
        "observation_counter = 0\n",
        "\n",
        "# Process both ground truth and observation HDF5 files\n",
        "for gt_file_name, obs_file_name in zip(ground_truth_files, observation_files):\n",
        "    gt_file_path = os.path.join(dataset_path, hdf5_folders[1], gt_file_name)\n",
        "    obs_file_path = os.path.join(dataset_path, hdf5_folders[4], obs_file_name)\n",
        "\n",
        "    # Try opening and processing the ground truth HDF5 file\n",
        "    try:\n",
        "        with h5py.File(gt_file_path, 'r') as gt_hdf5_file:\n",
        "            for j in range(len(gt_hdf5_file['data'])):\n",
        "                # Extract the ground truth image data\n",
        "                gt_data = gt_hdf5_file['data'][j]\n",
        "\n",
        "                # Save the ground truth image after rotating 90 degrees\n",
        "                gt_image_name = f'ground_truth_test_{\n",
        "                    ground_truth_counter:04d}.bmp'\n",
        "                plt.imsave(os.path.join(ground_truth_test_images_dir,\n",
        "                           gt_image_name), arr=np.rot90(gt_data, k=1), cmap='gray')\n",
        "                ground_truth_counter += 1\n",
        "\n",
        "        # Try opening and processing the corresponding observation HDF5 file\n",
        "        try:\n",
        "            with h5py.File(obs_file_path, 'r') as obs_hdf5_file:\n",
        "                for j in range(len(obs_hdf5_file['data'])):\n",
        "                    # Extract observation data and transpose for FBP reconstruction\n",
        "                    obs_data = obs_hdf5_file['data'][j]\n",
        "                    obs_data = np.transpose(obs_data)\n",
        "\n",
        "                    # Generate angles for FBP reconstruction\n",
        "                    theta = np.linspace(0., 180., max(\n",
        "                        obs_data.shape), endpoint=False)\n",
        "\n",
        "                    # Perform FBP (Filtered Back Projection) reconstruction\n",
        "                    reconstruction_fbp = iradon(\n",
        "                        obs_data, theta=theta, filter_name='ramp')\n",
        "\n",
        "                    # Crop the reconstructed image to a fixed size (362x362)\n",
        "                    crop_height = min(362, reconstruction_fbp.shape[0])\n",
        "                    crop_width = min(362, reconstruction_fbp.shape[1])\n",
        "                    start_row = (\n",
        "                        reconstruction_fbp.shape[0] - crop_height) // 2\n",
        "                    start_col = (reconstruction_fbp.shape[1] - crop_width) // 2\n",
        "                    reconstruction_fbp = reconstruction_fbp[start_row:start_row +\n",
        "                                                            crop_height, start_col:start_col + crop_width]\n",
        "\n",
        "                    # Save the observation image\n",
        "                    obs_image_name = f'observation_test_{\n",
        "                        observation_counter:04d}.bmp'\n",
        "                    plt.imsave(os.path.join(observation_test_images_dir,\n",
        "                               obs_image_name), arr=reconstruction_fbp, cmap='gray')\n",
        "                    observation_counter += 1\n",
        "\n",
        "        except OSError as e:\n",
        "            # Error handling for observation files\n",
        "            print(f\"Error opening {obs_file_path}: {e}\")\n",
        "            # Remove the corresponding ground truth image if observation fails\n",
        "            os.remove(os.path.join(ground_truth_test_images_dir, gt_image_name))\n",
        "            ground_truth_counter -= 1  # Adjust the counter to maintain consistency\n",
        "\n",
        "    except OSError as e:\n",
        "        # Error handling for ground truth files\n",
        "        print(f\"Error opening {gt_file_path}: {e}\")\n",
        "        # Remove the corresponding observation file if ground truth processing fails\n",
        "        if os.path.exists(obs_file_path):\n",
        "            os.remove(obs_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur1wvuaaS-49"
      },
      "source": [
        "## SRCNN as an ONNX framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ6kJZ-wYrci"
      },
      "source": [
        "This code defines a Super-Resolution Convolutional Neural Network (SRCNN) designed for single-channel (grayscale) images. The SRCNN model is a simple but effective deep learning model that performs image super-resolution by mapping low-resolution images to high-resolution counterparts through three convolutional layers. This script loads pre-trained weights for the SRCNN, sets the model to evaluation mode, and exports it to the ONNX format.\n",
        "\n",
        "Key components of the code:\n",
        "\n",
        "Model Definition: The SRCNN class defines a neural network with three convolutional layers. Each layer applies a different level of feature extraction and image refinement.\n",
        "\n",
        "The first layer uses 64 filters to extract features.\n",
        "The second layer reduces the feature space to 32 dimensions.\n",
        "The third layer outputs the reconstructed high-resolution image.\n",
        "ReLU activation is applied after the first two convolutional layers to introduce non-linearity.\n",
        "Loading Pre-Trained Weights: The script loads pre-trained weights (from a .pth file) into the SRCNN model. These weights are assumed to have been trained on an external dataset.\n",
        "\n",
        "Evaluation Mode: Once the model is loaded with weights, it's set to evaluation mode. This ensures that certain layers (like dropout or batch normalization, if they existed) behave properly during inference.\n",
        "\n",
        "Export to ONNX: The model is exported to the ONNX format using the torch.onnx.export function, with a dummy input tensor that simulates a grayscale (single-channel) image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHhMG3iQgfns"
      },
      "outputs": [],
      "source": [
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEO3Ug1Gf2dp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This script defines and exports a Super-Resolution Convolutional Neural Network (SRCNN) model to the ONNX format.\n",
        "SRCNN is designed for single-channel (grayscale) image input, and this implementation is based on the architecture\n",
        "outlined in the SRCNN paper for image super-resolution.\n",
        "\n",
        "The script includes the following steps:\n",
        "1. Define the SRCNN model architecture.\n",
        "2. Load pre-trained weights (state_dict) into the model.\n",
        "3. Set the model to evaluation mode.\n",
        "4. Export the model to ONNX format using a dummy single-channel image input for format specification.\n",
        "\n",
        "Dependencies:\n",
        "- torch: For defining and loading the SRCNN model and its weights.\n",
        "- torch.onnx: For exporting the PyTorch model to ONNX format.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.onnx\n",
        "\n",
        "# Define the SRCNN model class for single-channel input images\n",
        "\n",
        "\n",
        "class SRCNN(nn.Module):\n",
        "    def __init__(self, num_channels=1):\n",
        "        \"\"\"\n",
        "        Initializes the SRCNN model.\n",
        "\n",
        "        Args:\n",
        "            num_channels (int): Number of input channels (default is 1 for grayscale images).\n",
        "        \"\"\"\n",
        "        super(SRCNN, self).__init__()\n",
        "        # First convolutional layer with 64 filters, kernel size 9x9, and padding to keep output size same as input\n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n",
        "\n",
        "        # Second convolutional layer with 32 filters, kernel size 5x5, and padding to maintain spatial dimensions\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n",
        "\n",
        "        # Third convolutional layer that outputs the final single-channel (grayscale) image\n",
        "        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 // 2)\n",
        "\n",
        "        # ReLU activation function applied after the first two convolutional layers\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input image tensor of shape [batch_size, num_channels, height, width].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output high-resolution image tensor after passing through the network.\n",
        "        \"\"\"\n",
        "        # Pass input through the first convolutional layer followed by ReLU activation\n",
        "        x = self.relu(self.conv1(x))\n",
        "\n",
        "        # Pass through the second convolutional layer followed by ReLU activation\n",
        "        x = self.relu(self.conv2(x))\n",
        "\n",
        "        # Pass through the third convolutional layer to generate the final output image\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Create an instance of the SRCNN model for single-channel (grayscale) input\n",
        "model = SRCNN(num_channels=1)\n",
        "\n",
        "# Load the pre-trained model weights from the .pth file\n",
        "# Adjust the path to where the pre-trained weights are located\n",
        "state_dict = torch.load('/your/path/here/srcnn_x2.pth')\n",
        "model.load_state_dict(state_dict)  # Load the weights into the model\n",
        "\n",
        "# Set the model to evaluation mode to ensure correct behavior during inference (e.g., disabling dropout if present)\n",
        "model.eval()\n",
        "\n",
        "# Export the model to ONNX format\n",
        "torch.onnx.export(\n",
        "    model,                      # The model to be exported\n",
        "    # Dummy input of size [batch_size, num_channels, height, width] for grayscale images\n",
        "    torch.randn(1, 1, 224, 224),\n",
        "    \"/your/path/here/srcnn_x2.onnx\",    # Path where the ONNX model will be saved\n",
        "    export_params=True,          # Whether to store the trained weights inside the ONNX model\n",
        "    verbose=False,               # Whether to print detailed logs during export\n",
        "    opset_version=10             # ONNX opset version to ensure compatibility\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7t7ycgjV6ne"
      },
      "source": [
        "## Super-Resolution Model Evaluation Process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRtruYRnaBEp"
      },
      "source": [
        "This script is designed to evaluate the performance of multiple image super-resolution models by calculating several image quality metrics between the super-resolved images generated by these models and the ground truth high-resolution images. The models being evaluated include DAT, ESRGAN, HAT, SRCNN, and SwinIR, all of which aim to reconstruct high-quality images from lower-resolution observations.\n",
        "\n",
        "#### 1. **Input Preparation**\n",
        "\n",
        "- **Observation Images**: These are the low-resolution test images that will be super-resolved by the different models. The observation images are stored in a designated directory and follow a structured naming convention.\n",
        "- **Ground Truth Images**: The corresponding high-resolution images are used as references for comparison. These images are essential for calculating the quality metrics and are stored in a separate directory, also following a structured naming format.\n",
        "\n",
        "#### 2. **Super-Resolved Images from Models**\n",
        "\n",
        "- Each model has its own folder where the super-resolved images generated by that model are stored. These images are compared against the ground truth to determine how well each model reconstructs the high-resolution details from the low-resolution observations.\n",
        "\n",
        "#### 3. **Metrics Calculation**\n",
        "\n",
        "To assess the quality of the super-resolved images, the following metrics are calculated:\n",
        "\n",
        "- **PSNR (Peak Signal-to-Noise Ratio)**: This measures how closely the super-resolved image resembles the ground truth in terms of pixel intensity.\n",
        "- **SSIM (Structural Similarity Index)**: SSIM evaluates how similar the structure of the image is compared to the ground truth, focusing on contrast, luminance, and texture.\n",
        "- **LPIPS (Learned Perceptual Image Patch Similarity)**: LPIPS assesses the perceptual quality of the image, accounting for how humans perceive image differences, using deep learning.\n",
        "- **NIQE (Natural Image Quality Evaluator)**: This is a no-reference metric, meaning it doesn’t require a ground truth image. It assesses the quality of the super-resolved image based on statistical properties.\n",
        "- **NRQM (No-Reference Quality Metric)**: Like NIQE, this is a no-reference metric but focuses on perceptual image quality without requiring a reference image.\n",
        "- **PI (Perceptual Index)**: This is a combination of the NIQE and NRQM scores and provides an overall perceptual quality index for the image.\n",
        "  The metric values, along with the computation times, are recorded for analysis.\n",
        "\n",
        "#### 4. **Processing Flow**\n",
        "\n",
        "- **Preprocessing**: The observation and ground truth images are loaded from their respective directories and converted to arrays for further processing.\n",
        "- **Metric Computation**: For each pair of images (observation/model output vs. ground truth), the defined metrics are computed. In some cases, like LPIPS, NIQE, and NRQM, the images are first converted to PyTorch tensors.\n",
        "- **Result Storage**: The calculated metric values, along with the corresponding image IDs and model names, are saved to a structured list. Additionally, the runtime for each metric calculation is recorded for performance analysis.\n",
        "\n",
        "#### 5. **Data Export**\n",
        "\n",
        "- Once all images and models are processed, the results are compiled into a pandas DataFrame, which is then exported as a CSV file. This CSV contains detailed information on each metric for each image and model, providing a comprehensive evaluation of the models’ performance.\n",
        "\n",
        "#### 6. **Output Analysis**\n",
        "\n",
        "- The final DataFrame allows for further analysis of how each model performs across different images and metrics. By analyzing this data, one can determine which models excel in different aspects of super-resolution, such as perceptual quality, structural accuracy, or overall similarity to the ground truth.\n",
        "\n",
        "This evaluation process not only provides quantitative insights into each model's performance but also highlights trade-offs between various metrics, allowing for informed decisions when choosing a model for specific tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVkWSRfxelxt"
      },
      "outputs": [],
      "source": [
        "!pip install lpips\n",
        "!pip install pyiqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUSNDyG4V9BO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This script evaluates the performance of different image super-resolution models\n",
        "by calculating various image quality metrics, such as PSNR, SSIM, LPIPS, NIQE, NRQM,\n",
        "and Perceptual Index (PI), between the super-resolved images generated by the models\n",
        "and the ground truth high-resolution images.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import skimage.metrics\n",
        "import torch\n",
        "from lpips import LPIPS\n",
        "import pyiqa\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Paths to the folders containing the observation images, ground truth images, and model-generated images.\n",
        "observation_path = 'your/path/here/Dataset/observation_test_images'\n",
        "ground_truth_path = 'your/path/here/Dataset/ground_truth_test_images'\n",
        "model_paths = {\n",
        "    'DAT': 'your/path/here/Images/Observation/DAT',\n",
        "    'ESRGAN': 'your/path/here/Images/Observation/ESRGAN',\n",
        "    'HAT': 'your/path/here/Images/Observation/HAT',\n",
        "    'SRCNN': 'your/path/here/Images/Observation/SRCNN',\n",
        "    'SwinIR': 'your/path/here/Images/Observation/SwinIR'\n",
        "}\n",
        "\n",
        "# Define the metrics to calculate for evaluation\n",
        "metrics = ['PSNR', 'SSIM', 'LPIPS', 'NIQE', 'NRQM', 'PI']\n",
        "\n",
        "# Initialize the LPIPS model for perceptual similarity (AlexNet backbone)\n",
        "lpips_model = LPIPS(net='alex')\n",
        "\n",
        "# Initialize the NIQE and NRQM models using pyiqa for no-reference image quality assessments\n",
        "niqe_model = pyiqa.create_metric('niqe')\n",
        "nrqm_model = pyiqa.create_metric('nrqm')\n",
        "\n",
        "\n",
        "def pil_to_tensor(image):\n",
        "    \"\"\"\n",
        "    Converts a PIL image to a PyTorch tensor.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input PIL image.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Image converted to a 4D tensor with shape (1, C, H, W).\n",
        "    \"\"\"\n",
        "    image = np.array(image).astype(np.float32) / \\\n",
        "        255.0  # Normalize to [0, 1] range\n",
        "    image = torch.tensor(image).permute(2, 0, 1)  # Change to [C, H, W] format\n",
        "    image = image.unsqueeze(0)  # Add batch dimension [1, C, H, W]\n",
        "    return image\n",
        "\n",
        "\n",
        "def calculate_pi(niqe, nrqm):\n",
        "    \"\"\"\n",
        "    Calculates the Perceptual Index (PI) metric based on NIQE and NRQM scores.\n",
        "\n",
        "    PI = (10 - NRQM) / 2 + NIQE / 2\n",
        "\n",
        "    Args:\n",
        "        niqe (float): NIQE score of the image.\n",
        "        nrqm (float): NRQM score of the image.\n",
        "\n",
        "    Returns:\n",
        "        float: Perceptual Index score.\n",
        "    \"\"\"\n",
        "    return (10 - nrqm) / 2 + niqe / 2\n",
        "\n",
        "\n",
        "def calculate_metrics(image1, image2):\n",
        "    \"\"\"\n",
        "    Calculates multiple image quality metrics (PSNR, SSIM, LPIPS, NIQE, NRQM, PI)\n",
        "    between two images.\n",
        "\n",
        "    Args:\n",
        "        image1 (np.array): Ground truth image as a NumPy array.\n",
        "        image2 (np.array): Test or model-generated image as a NumPy array.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with metric names as keys and tuples of (metric_value, runtime) as values.\n",
        "    \"\"\"\n",
        "    metrics_data = {}\n",
        "\n",
        "    # PSNR (Peak Signal-to-Noise Ratio)\n",
        "    start_time = time.time()\n",
        "    psnr = skimage.metrics.peak_signal_noise_ratio(image1, image2)\n",
        "    metrics_data['PSNR'] = (psnr, time.time() - start_time)\n",
        "\n",
        "    # SSIM (Structural Similarity Index)\n",
        "    start_time = time.time()\n",
        "    ssim = skimage.metrics.structural_similarity(\n",
        "        image1, image2, channel_axis=-1)\n",
        "    metrics_data['SSIM'] = (ssim, time.time() - start_time)\n",
        "\n",
        "    # Convert images to tensors for LPIPS, NIQE, and NRQM calculation\n",
        "    observation_tensor = pil_to_tensor(observation_img)\n",
        "    ground_truth_tensor = pil_to_tensor(ground_truth_img)\n",
        "\n",
        "    # LPIPS (Learned Perceptual Image Patch Similarity)\n",
        "    start_time = time.time()\n",
        "    lpips_score = lpips_model(ground_truth_tensor, observation_tensor).item()\n",
        "    metrics_data['LPIPS'] = (lpips_score, time.time() - start_time)\n",
        "\n",
        "    # NIQE (Natural Image Quality Evaluator)\n",
        "    start_time = time.time()\n",
        "    niqe_score = niqe_model(observation_tensor).item()\n",
        "    metrics_data['NIQE'] = (niqe_score, time.time() - start_time)\n",
        "\n",
        "    # NRQM (No-Reference Quality Metric)\n",
        "    start_time = time.time()\n",
        "    nrqm_score = nrqm_model(observation_tensor).item()\n",
        "    metrics_data['NRQM'] = (nrqm_score, time.time() - start_time)\n",
        "\n",
        "    # Perceptual Index (PI)\n",
        "    start_time = time.time()\n",
        "    pi_score = calculate_pi(niqe_score, nrqm_score)\n",
        "    metrics_data['PI'] = (pi_score, time.time() - start_time)\n",
        "\n",
        "    return metrics_data\n",
        "\n",
        "\n",
        "# Get a list of all observation images in the dataset and sort by filename to ensure consistency\n",
        "observation_images = glob.glob(os.path.join(observation_path, '*.bmp'))\n",
        "observation_images.sort()\n",
        "\n",
        "# Select the first 150 observation images\n",
        "selected_observation_images = observation_images[:150]\n",
        "\n",
        "# Extract the image IDs from the filenames (assuming filenames follow a specific format)\n",
        "image_ids = [int(os.path.basename(image).split('.')[0].split('_')[-1])\n",
        "             for image in selected_observation_images]\n",
        "\n",
        "# Initialize an empty list to store results\n",
        "data = []\n",
        "\n",
        "# Process the observation images and calculate metrics\n",
        "for observation_image, observation_number in zip(selected_observation_images, image_ids):\n",
        "    # Find the corresponding ground truth image\n",
        "    ground_truth_image = os.path.join(\n",
        "        ground_truth_path,\n",
        "        f'ground_truth_test_{observation_number:04d}.bmp'\n",
        "    )\n",
        "\n",
        "    # Open the observation and ground truth images as PIL images\n",
        "    observation_img = Image.open(observation_image)\n",
        "    ground_truth_img = Image.open(ground_truth_image)\n",
        "\n",
        "    # Convert the images to NumPy arrays\n",
        "    observation_array = np.array(observation_img)\n",
        "    ground_truth_array = np.array(ground_truth_img)\n",
        "\n",
        "    # Calculate metrics for the observation image\n",
        "    observation_metrics = calculate_metrics(\n",
        "        ground_truth_array, observation_array)\n",
        "    for metric, (value, runtime) in observation_metrics.items():\n",
        "        data.append([observation_number, 'Observation',\n",
        "                    metric, value, runtime])\n",
        "\n",
        "# Process images from each model and calculate metrics\n",
        "for model, model_path in model_paths.items():\n",
        "    for observation_number in image_ids:\n",
        "        # Find the corresponding model-generated and ground truth images\n",
        "        model_image = os.path.join(\n",
        "            model_path,\n",
        "            f'observation_test_{observation_number:04d}.bmp'\n",
        "        )\n",
        "        ground_truth_image = os.path.join(\n",
        "            ground_truth_path,\n",
        "            f'ground_truth_test_{observation_number:04d}.bmp'\n",
        "        )\n",
        "\n",
        "        # Open the model and ground truth images as PIL images\n",
        "        observation_img = Image.open(model_image)\n",
        "        ground_truth_img = Image.open(ground_truth_image)\n",
        "\n",
        "        # Convert the images to NumPy arrays\n",
        "        observation_array = np.array(observation_img)\n",
        "        ground_truth_array = np.array(ground_truth_img)\n",
        "\n",
        "        # Calculate metrics for the model-generated image\n",
        "        model_metrics = calculate_metrics(\n",
        "            ground_truth_array, observation_array)\n",
        "        for metric, (value, runtime) in model_metrics.items():\n",
        "            data.append([observation_number, model, metric, value, runtime])\n",
        "\n",
        "# Convert the collected data into a DataFrame for analysis and export\n",
        "df = pd.DataFrame(\n",
        "    data, columns=['Image_ID', 'Model', 'Metric', 'Value', 'Runtime'])\n",
        "\n",
        "# Save the DataFrame to a CSV file for further use\n",
        "df.to_csv('your/path/here/metrics_results.csv', index=False)\n",
        "\n",
        "# Print the DataFrame to view the results\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56V6hRjAWZ7F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('your/path/here/metrics_results.csv')\n",
        "\n",
        "# Group by 'Model' and 'Metric' and calculate the mean and standard deviation of 'Value'\n",
        "aggregated_df = df.groupby(['Model', 'Metric']).agg(\n",
        "    {'Value': ['mean', 'std']}\n",
        ")\n",
        "\n",
        "# Print the aggregated DataFrame\n",
        "print(aggregated_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5C7OSt3WjvI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('your/path/here/metrics_results.csv')\n",
        "\n",
        "# Group by 'Model' and 'Metric' and calculate the mean and standard deviation of 'Value'\n",
        "aggregated_df = df.groupby(['Model', 'Metric']).agg(\n",
        "    {'Value': ['mean', 'std']}\n",
        ")\n",
        "\n",
        "# Get the unique metrics\n",
        "metrics = df['Metric'].unique()\n",
        "\n",
        "# Initialize the dictionary to store the best models\n",
        "best_results = {}\n",
        "\n",
        "# Iterate over each metric\n",
        "for metric in metrics:\n",
        "    # If the metric is PSNR, SSIM, or NRQM, find the model with the highest mean value\n",
        "    if metric in ['PSNR', 'SSIM', 'NRQM']:\n",
        "        best_model = aggregated_df.loc[(\n",
        "            slice(None), metric), ('Value', 'mean')].idxmax()[0]\n",
        "    # Otherwise, find the model with the lowest mean value\n",
        "    else:\n",
        "        best_model = aggregated_df.loc[(\n",
        "            slice(None), metric), ('Value', 'mean')].idxmin()[0]\n",
        "    # Store the best model for the current metric\n",
        "    best_results[metric] = best_model\n",
        "\n",
        "# Print the best models for each metric\n",
        "print('Best results:')\n",
        "for metric, model in best_results.items():\n",
        "    print(f'{metric}: {model}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFWvy9ZvaTPb"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv('metrics_results (2).csv')\n",
        "\n",
        "# Group by 'Model' and 'Metric' and calculate the mean and standard deviation of 'Value'\n",
        "aggregated_df = df.groupby(['Model', 'Metric']).agg(\n",
        "    {'Value': ['mean', 'std']}\n",
        ")\n",
        "\n",
        "# Print the aggregated DataFrame\n",
        "print(aggregated_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc2Yc9PlqWlv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/metrics_results (2).csv')\n",
        "\n",
        "# Set up the grid for 3 rows and 2 columns\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Define the metrics\n",
        "metrics = df['Metric'].unique()\n",
        "\n",
        "# Loop through each metric\n",
        "for i, metric in enumerate(metrics):\n",
        "    # Select the axis\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Filter the data for the current metric\n",
        "    data = df[df['Metric'] == metric]\n",
        "\n",
        "    # Create the boxplot\n",
        "    boxplot = sns.boxplot(x='Model', y='Value', data=data, ax=ax)\n",
        "\n",
        "    # Customize colors for specific models\n",
        "    for j, artist in enumerate(ax.patches):\n",
        "        # Extract the corresponding model for the current box\n",
        "        model = data['Model'].unique()[j]\n",
        "\n",
        "        if model == 'ESRGAN':\n",
        "            artist.set_facecolor('lightgreen')\n",
        "\n",
        "    # Set titles and labels\n",
        "    ax.set_title(f'Diagrama de Caixa da {metric}')\n",
        "    ax.set_xlabel('Modelo')\n",
        "    ax.set_ylabel('Valor')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEkymIG4WMPs"
      },
      "source": [
        "## Sample plot for visual comparision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-27ANYZiaWjl"
      },
      "source": [
        "This script visualizes the super-resolved images generated by various models (DAT, ESRGAN, HAT, SRCNN, and SwinIR) alongside the low-resolution observation image and the corresponding ground truth high-resolution image. The process involves loading images from pre-specified directories, displaying them in a side-by-side comparison, and saving these visualizations as high-resolution PNG files.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Define Paths: Paths to the observation, ground truth, and model-generated images are specified. Each model has its own directory where its super-resolved images are stored.\n",
        "Image Loading: The observation image and the ground truth image are located and loaded. Similarly, for each model, the super-resolved images corresponding to the observation are loaded.\n",
        "Visualization: Two sets of visualizations are created:\n",
        "The ground truth image is saved individually.\n",
        "The observation and model images are displayed in a 2x3 grid for comparison, with the observation image and each model’s output in its own subplot.\n",
        "Saving and Displaying: The plots are saved as PNG files with high resolution, and the second plot (observation + model images) is displayed for immediate inspection.\n",
        "This approach allows for a clear visual comparison of how different models reconstruct the super-resolved images relative to the ground truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hT6oPY9WP17"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def visualize_model_comparisons(image_id):\n",
        "    \"\"\"\n",
        "    Visualizes the super-resolved images generated by different models, comparing them\n",
        "    with the observation image and the ground truth image. Saves the visualizations\n",
        "    as PNG files.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    image_id : int\n",
        "        The ID of the image to visualize (used for both observation and ground truth images).\n",
        "\n",
        "    Process:\n",
        "    1. Load the observation image and ground truth image.\n",
        "    2. Load the super-resolved images from five different models: DAT, ESRGAN, HAT, SRCNN, and SwinIR.\n",
        "    3. Display the ground truth image individually and save it.\n",
        "    4. Create a 2x3 grid to visualize the observation image alongside the super-resolved images from each model.\n",
        "    5. Save the comparison as a PNG image and display the plot.\n",
        "    \"\"\"\n",
        "    # Define the paths to the different folders\n",
        "    observation_path = 'your/path/here/Dataset/observation_test_images'\n",
        "    ground_truth_path = 'your/path/here/Dataset/ground_truth_test_images'\n",
        "    model_paths = {\n",
        "        'DAT': 'your/path/here/Images/DAT',\n",
        "        'ESRGAN': 'your/path/here/Images/ESRGAN',\n",
        "        'HAT': 'your/path/here/Images/HAT',\n",
        "        'SRCNN': 'your/path/here/Images/SRCNN',\n",
        "        'SwinIR': 'your/path/here/Images/SwinIR'\n",
        "    }\n",
        "\n",
        "    # Find and load the observation image\n",
        "    observation_image = os.path.join(\n",
        "        observation_path, f'observation_test_{image_id:04d}.bmp')\n",
        "    observation_img = Image.open(observation_image)\n",
        "\n",
        "    # Find and load the ground truth image\n",
        "    ground_truth_image = os.path.join(\n",
        "        ground_truth_path, f'ground_truth_test_{image_id:04d}.bmp')\n",
        "    ground_truth_img = Image.open(ground_truth_image)\n",
        "\n",
        "    # Find and load the model-generated images\n",
        "    model_images = {}\n",
        "    for model, model_path in model_paths.items():\n",
        "        model_image = os.path.join(\n",
        "            model_path, f'observation_test_{image_id:04d}.bmp')\n",
        "        model_images[model] = Image.open(model_image)\n",
        "\n",
        "    # Save the ground truth image individually\n",
        "    fig_gt, ax_gt = plt.subplots(figsize=(5, 5))\n",
        "    ax_gt.imshow(ground_truth_img)\n",
        "    ax_gt.set_title('Ground Truth', fontsize=14)\n",
        "    ax_gt.axis('off')\n",
        "    plt.savefig(f'ground_truth_image_{\n",
        "                image_id}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig_gt)  # Close the figure after saving\n",
        "\n",
        "    # Create a new figure for the remaining images (Observation + Models)\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "    # Flatten the array of axes for easier iteration\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    # Define a consistent font size for titles\n",
        "    title_fontsize = 14\n",
        "\n",
        "    # Display the observation image\n",
        "    axs[0].imshow(observation_img)\n",
        "    axs[0].set_title('Observation', fontsize=title_fontsize)\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Display the model images\n",
        "    for i, (model, image) in enumerate(model_images.items()):\n",
        "        axs[i+1].imshow(image)\n",
        "        axs[i+1].set_title(model, fontsize=title_fontsize)\n",
        "        axs[i+1].axis('off')\n",
        "\n",
        "    # Turn off any unused axes\n",
        "    for j in range(len(model_images) + 1, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    # Adjust spacing between the subplots\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
        "\n",
        "    # Save the comparison figure with high resolution\n",
        "    plt.savefig(f'comparison_models_image_{\n",
        "                image_id}.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Show the plot for inspection\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "visualize_model_comparisons(image_id=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbSeaZfQNMmJ"
      },
      "outputs": [],
      "source": [
        "# prompt: Get the metrics for image_id=16 from metrics_results.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/metrics_results.csv')\n",
        "\n",
        "# Filter the DataFrame for image_id = 16\n",
        "image_16_metrics = df[df['Image_ID'] == 16]\n",
        "\n",
        "# Print the metrics for image_id = 16\n",
        "print(image_16_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JngiGnkwKZTa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/metrics_results.csv')\n",
        "\n",
        "# Define metrics where higher values are better\n",
        "higher_is_better_metrics = ['PSNR', 'SSIM', 'NRQM']\n",
        "\n",
        "# Define metrics where lower values are better\n",
        "lower_is_better_metrics = ['LPIPS', 'NIQE', 'PI']\n",
        "\n",
        "# Normalize metrics\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply normalization based on whether higher or lower values are better\n",
        "for metric in higher_is_better_metrics + lower_is_better_metrics:\n",
        "    df_metric = df[df['Metric'] == metric]\n",
        "    df_metric[['Value']] = scaler.fit_transform(df_metric[['Value']])\n",
        "\n",
        "    if metric in lower_is_better_metrics:\n",
        "        df_metric[['Value']] = 1 - df_metric[['Value']]\n",
        "\n",
        "    df.update(df_metric)\n",
        "\n",
        "# Group by 'Model' and 'Metric' and calculate the mean of 'Value'\n",
        "aggregated_df = df.groupby(['Model', 'Metric'])['Value'].mean().reset_index()\n",
        "\n",
        "# Pivot the DataFrame to have 'Metric' as columns and 'Model' as rows\n",
        "pivot_df = aggregated_df.pivot(index='Metric', columns='Model', values='Value')\n",
        "\n",
        "# Calculate the difference between ESRGAN and Observation for each metric\n",
        "pivot_df['diff'] = pivot_df['ESRGAN'] - pivot_df['Observation']\n",
        "\n",
        "# Calculate the sum of the differences for each image ID\n",
        "df['diff_sum'] = df.groupby('Image_ID')['Value'].transform(\n",
        "    lambda x: x.sum() if df['Model'].iloc[0] == 'ESRGAN' else -x.sum())\n",
        "\n",
        "# Sort the IDs by 'diff_sum' and get the second-largest difference\n",
        "sorted_diff_df = df.groupby('Image_ID')['diff_sum'].sum().reset_index()\n",
        "sorted_diff_df = sorted_diff_df.sort_values(by='diff_sum', ascending=False)\n",
        "\n",
        "# Get the second-largest difference\n",
        "second_max_diff_id = sorted_diff_df.iloc[1]['Image_ID']\n",
        "\n",
        "print(f\"The ID with the second-largest difference is: {second_max_diff_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ChJzgpYLVPK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def visualize_model_comparisons(image_id):\n",
        "    \"\"\"\n",
        "    Visualizes the super-resolved images generated by different models, comparing them\n",
        "    with the observation image and the ground truth image. Saves the visualizations\n",
        "    as PNG files.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    image_id : int\n",
        "        The ID of the image to visualize (used for both observation and ground truth images).\n",
        "\n",
        "    Process:\n",
        "    1. Load the observation image and ground truth image.\n",
        "    2. Load the super-resolved images from five different models: DAT, ESRGAN, HAT, SRCNN, and SwinIR.\n",
        "    3. Display the ground truth image individually and save it.\n",
        "    4. Create a 2x3 grid to visualize the observation image alongside the super-resolved images from each model.\n",
        "    5. Save the comparison as a PNG image and display the plot.\n",
        "    \"\"\"\n",
        "    # Define the paths to the different folders\n",
        "    observation_path = 'your/path/here/Dataset/observation_test_images'\n",
        "    ground_truth_path = 'your/path/here/Dataset/ground_truth_test_images'\n",
        "    model_paths = {\n",
        "        'DAT': 'your/path/here/Images/DAT',\n",
        "        'ESRGAN': 'your/path/here/Images/ESRGAN',\n",
        "        'HAT': 'your/path/here/Images/HAT',\n",
        "        'SRCNN': 'your/path/here/Images/SRCNN',\n",
        "        'SwinIR': 'your/path/here/Images/SwinIR'\n",
        "    }\n",
        "\n",
        "    # Find and load the observation image\n",
        "    observation_image = os.path.join(\n",
        "        observation_path, f'observation_test_{image_id:04d}.bmp')\n",
        "    observation_img = Image.open(observation_image)\n",
        "\n",
        "    # Find and load the ground truth image\n",
        "    ground_truth_image = os.path.join(\n",
        "        ground_truth_path, f'ground_truth_test_{image_id:04d}.bmp')\n",
        "    ground_truth_img = Image.open(ground_truth_image)\n",
        "\n",
        "    # Find and load the model-generated images\n",
        "    model_images = {}\n",
        "    for model, model_path in model_paths.items():\n",
        "        model_image = os.path.join(\n",
        "            model_path, f'observation_test_{image_id:04d}.bmp')\n",
        "        model_images[model] = Image.open(model_image)\n",
        "\n",
        "    # Save the ground truth image individually\n",
        "    fig_gt, ax_gt = plt.subplots(figsize=(5, 5))\n",
        "    ax_gt.imshow(ground_truth_img)\n",
        "    ax_gt.set_title('Ground Truth', fontsize=14)\n",
        "    ax_gt.axis('off')\n",
        "    plt.savefig(f'ground_truth_image_{\n",
        "                image_id}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig_gt)  # Close the figure after saving\n",
        "\n",
        "    # Create a new figure for the remaining images (Observation + Models)\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "    # Flatten the array of axes for easier iteration\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    # Define a consistent font size for titles\n",
        "    title_fontsize = 14\n",
        "\n",
        "    # Display the observation image\n",
        "    axs[0].imshow(observation_img)\n",
        "    axs[0].set_title('Observation', fontsize=title_fontsize)\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Display the model images\n",
        "    for i, (model, image) in enumerate(model_images.items()):\n",
        "        axs[i+1].imshow(image)\n",
        "        axs[i+1].set_title(model, fontsize=title_fontsize)\n",
        "        axs[i+1].axis('off')\n",
        "\n",
        "    # Turn off any unused axes\n",
        "    for j in range(len(model_images) + 1, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    # Adjust spacing between the subplots\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
        "\n",
        "    # Save the comparison figure with high resolution\n",
        "    plt.savefig(f'comparison_models_image_{\n",
        "                image_id}.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Show the plot for inspection\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "visualize_model_comparisons(image_id=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOJIIAxCK5YJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/metrics_results (2).csv')\n",
        "\n",
        "# Define metrics where higher values are better\n",
        "higher_is_better_metrics = ['PSNR', 'SSIM', 'NRQM']\n",
        "\n",
        "# Define metrics where lower values are better\n",
        "lower_is_better_metrics = ['LPIPS', 'NIQE', 'PI']\n",
        "\n",
        "# Normalize metrics\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply normalization based on whether higher or lower values are better\n",
        "for metric in higher_is_better_metrics + lower_is_better_metrics:\n",
        "    df_metric = df[df['Metric'] == metric]\n",
        "    df_metric[['Value']] = scaler.fit_transform(df_metric[['Value']])\n",
        "\n",
        "    if metric in lower_is_better_metrics:\n",
        "        df_metric[['Value']] = 1 - df_metric[['Value']]\n",
        "\n",
        "    df.update(df_metric)\n",
        "\n",
        "# Group by 'Model' and 'Metric' and calculate the mean of 'Value'\n",
        "aggregated_df = df.groupby(['Model', 'Metric'])['Value'].mean().reset_index()\n",
        "\n",
        "# Pivot the DataFrame to have 'Metric' as columns and 'Model' as rows\n",
        "pivot_df = aggregated_df.pivot(index='Metric', columns='Model', values='Value')\n",
        "\n",
        "# Calculate the difference between each model and the 'Observation' model for each metric\n",
        "for model in pivot_df.columns:\n",
        "    if model != 'Observation':\n",
        "        pivot_df[f'diff_{model}'] = pivot_df[model] - pivot_df['Observation']\n",
        "\n",
        "# Calculate the sum of the absolute differences for each image ID\n",
        "df['diff_sum'] = df.groupby('Image_ID').apply(lambda x: (\n",
        "    x['Value'] - x[x['Model'] == 'Observation']['Value'].mean()).abs().sum()).reset_index(level=0, drop=True)\n",
        "\n",
        "# Find the ID with the smallest difference\n",
        "min_diff_id = df.loc[df['diff_sum'].idxmin(), 'Image_ID']\n",
        "\n",
        "print(f\"The ID with the smallest difference is: {min_diff_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Er-MIh3LWZr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def visualize_model_comparisons(image_id):\n",
        "    \"\"\"\n",
        "    Visualizes the super-resolved images generated by different models, comparing them\n",
        "    with the observation image and the ground truth image. Saves the visualizations\n",
        "    as PNG files.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    image_id : int\n",
        "        The ID of the image to visualize (used for both observation and ground truth images).\n",
        "\n",
        "    Process:\n",
        "    1. Load the observation image and ground truth image.\n",
        "    2. Load the super-resolved images from five different models: DAT, ESRGAN, HAT, SRCNN, and SwinIR.\n",
        "    3. Display the ground truth image individually and save it.\n",
        "    4. Create a 2x3 grid to visualize the observation image alongside the super-resolved images from each model.\n",
        "    5. Save the comparison as a PNG image and display the plot.\n",
        "    \"\"\"\n",
        "    # Define the paths to the different folders\n",
        "    observation_path = 'your/path/here/Dataset/observation_test_images'\n",
        "    ground_truth_path = 'your/path/here/Dataset/ground_truth_test_images'\n",
        "    model_paths = {\n",
        "        'DAT': 'your/path/here/Images/DAT',\n",
        "        'ESRGAN': 'your/path/here/Images/ESRGAN',\n",
        "        'HAT': 'your/path/here/Images/HAT',\n",
        "        'SRCNN': 'your/path/here/Images/SRCNN',\n",
        "        'SwinIR': 'your/path/here/Images/SwinIR'\n",
        "    }\n",
        "\n",
        "    # Define the paths to the different folders\n",
        "    observation_path = f'{path}/Dataset/observation_test_images'\n",
        "    ground_truth_path = f'{path}/Dataset/ground_truth_test_images'\n",
        "    model_paths = {\n",
        "        'DAT': f'{path}/Images/Observation/DAT',\n",
        "        'ESRGAN': f'{path}/Images/Observation/ESRGAN',\n",
        "        'HAT': f'{path}/Images/Observation/HAT',\n",
        "        'SRCNN': f'{path}/Images/Observation/SRCNN',\n",
        "        'SwinIR': f'{path}/Images/Observation/SwinIR'\n",
        "    }\n",
        "\n",
        "    # Find and load the observation image\n",
        "    observation_image = os.path.join(\n",
        "        observation_path, f'observation_test_{image_id:04d}.bmp')\n",
        "    observation_img = Image.open(observation_image)\n",
        "\n",
        "    # Find and load the ground truth image\n",
        "    ground_truth_image = os.path.join(\n",
        "        ground_truth_path, f'ground_truth_test_{image_id:04d}.bmp')\n",
        "    ground_truth_img = Image.open(ground_truth_image)\n",
        "\n",
        "    # Find and load the model-generated images\n",
        "    model_images = {}\n",
        "    for model, model_path in model_paths.items():\n",
        "        model_image = os.path.join(\n",
        "            model_path, f'observation_test_{image_id:04d}.bmp')\n",
        "        model_images[model] = Image.open(model_image)\n",
        "\n",
        "    # Save the ground truth image individually\n",
        "    fig_gt, ax_gt = plt.subplots(figsize=(5, 5))\n",
        "    ax_gt.imshow(ground_truth_img)\n",
        "    ax_gt.set_title('Ground Truth', fontsize=14)\n",
        "    ax_gt.axis('off')\n",
        "    plt.savefig(f'ground_truth_image_{\n",
        "                image_id}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig_gt)  # Close the figure after saving\n",
        "\n",
        "    # Create a new figure for the remaining images (Observation + Models)\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "    # Flatten the array of axes for easier iteration\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    # Define a consistent font size for titles\n",
        "    title_fontsize = 14\n",
        "\n",
        "    # Display the observation image\n",
        "    axs[0].imshow(observation_img)\n",
        "    axs[0].set_title('Observation', fontsize=title_fontsize)\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Display the model images\n",
        "    for i, (model, image) in enumerate(model_images.items()):\n",
        "        axs[i+1].imshow(image)\n",
        "        axs[i+1].set_title(model, fontsize=title_fontsize)\n",
        "        axs[i+1].axis('off')\n",
        "\n",
        "    # Turn off any unused axes\n",
        "    for j in range(len(model_images) + 1, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    # Adjust spacing between the subplots\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
        "\n",
        "    # Save the comparison figure with high resolution\n",
        "    plt.savefig(f'comparison_models_image_{\n",
        "                image_id}.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Show the plot for inspection\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "visualize_model_comparisons(image_id=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Tp3GQAhTdR"
      },
      "outputs": [],
      "source": [
        "# prompt: Get the metrics for image_id=1 from metrics_results.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/metrics_results (2).csv')\n",
        "\n",
        "# Filter the DataFrame for image_id = 1\n",
        "image_1_metrics = df[df['Image_ID'] == 1]\n",
        "\n",
        "# Print the metrics for image_id = 1\n",
        "print(image_1_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqgPeT7FRDr-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/metrics_results (2).csv')\n",
        "\n",
        "# Define metrics where higher values are better\n",
        "higher_is_better_metrics = ['PSNR', 'SSIM', 'NRQM']\n",
        "\n",
        "# Define metrics where lower values are better\n",
        "lower_is_better_metrics = ['LPIPS', 'NIQE', 'PI']\n",
        "\n",
        "# Normalize metrics\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply normalization based on whether higher or lower values are better\n",
        "for metric in higher_is_better_metrics + lower_is_better_metrics:\n",
        "    df_metric = df[df['Metric'] == metric]\n",
        "    df_metric[['Value']] = scaler.fit_transform(df_metric[['Value']])\n",
        "\n",
        "    if metric in lower_is_better_metrics:\n",
        "        df_metric[['Value']] = 1 - df_metric[['Value']]\n",
        "\n",
        "    df.update(df_metric)\n",
        "\n",
        "# Create a DataFrame to store whether Observation is better for each ID\n",
        "better_observation_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over each unique Image_ID\n",
        "for image_id in df['Image_ID'].unique():\n",
        "    # Filter data for the current Image_ID\n",
        "    id_df = df[df['Image_ID'] == image_id]\n",
        "\n",
        "    # Create a pivot table to compare Observation with other models\n",
        "    pivot_df = id_df.pivot(index='Metric', columns='Model', values='Value')\n",
        "\n",
        "    if 'Observation' in pivot_df.columns:\n",
        "        # Check if Observation is better than all other models for each metric\n",
        "        all_better = True\n",
        "        for model in pivot_df.columns:\n",
        "            if model != 'Observation':\n",
        "                # Check if Observation is better than this model for all metrics\n",
        "                if not ((pivot_df['Observation'] >= pivot_df[model]).all() or\n",
        "                        (pivot_df['Observation'] <= pivot_df[model]).all()):\n",
        "                    all_better = False\n",
        "                    break\n",
        "\n",
        "        # Record the result\n",
        "        better_observation_df = pd.concat([better_observation_df, pd.DataFrame(\n",
        "            {'Image_ID': [image_id], 'Observation_Better': [all_better]})], ignore_index=True)\n",
        "\n",
        "# Find IDs where Observation is better than all models\n",
        "better_observation_ids = better_observation_df[better_observation_df['Observation_Better']]['Image_ID']\n",
        "\n",
        "print(f\"IDs where Observation wins against all models: {\n",
        "      list(better_observation_ids)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cornerstone Project - Eric Rodrigues de Carvalho\n",
    "\n",
    "This notebook is a series of codes that serves as one of the primary tools to execute my cornerstone project as an undergraduated student called \"SUPER-RESOLUÇÃO DE IMAGENS EM\n",
    "TOMOGRAFIA COMPUTADORIZADA DE BAIXA\n",
    "DOSAGEM: COMPARAÇÃO DE MÉTODOS DE\n",
    "APRENDIZADO PROFUNDO\" (SUPER-RESOLUTION OF IMAGES IN\n",
    "COMPUTED TOMOGRAPHY OF LOW\n",
    "DOSAGE: COMPARISON OF METHODS OF\n",
    "DEEP LEARNING).\n",
    "\n",
    "The main goals of this notebook are:\n",
    "\n",
    "1. Instruct how to install the required Python packages, download and prepare the Dataset and Models used in the work. (Section 0)\n",
    "2. Convert files from HDF5 (the source file) to actual PNG images. (Section 1)\n",
    "3. Convert sinogram representation to actual and human-readable CT-scanned images. (Section 1)\n",
    "4. Convert SRCNN model to ONNX-based in order to run in chaiNNer application. (Section 2)\n",
    "5. Instruct how to use chaiNNer to reproduce our results. (Section 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0 - Python and Data Preparation\n",
    "\n",
    "##### Requirements \n",
    "This repository require the following packages\n",
    "\n",
    "- h5py \n",
    "- numpy \n",
    "- matplotlib \n",
    "- scikit-learn\n",
    "- scikit-image\n",
    "- onnx\n",
    "- torch\n",
    "- lpips\n",
    "- pyiqa\n",
    "- seaborn\n",
    "\n",
    "Install them (and their requirements) using pip, or: \n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "##### Data preparation\n",
    "\n",
    "The data used in this work was downloaded from [Zenodo](https://zenodo.org/records/3384092).\n",
    "\n",
    "Two files were used:\n",
    "\n",
    "- observation_test.zip ([Link](https://zenodo.org/records/3384092/files/observation_validation.zip?download=1))\n",
    "- ground_truth_test.zip ([Link](https://zenodo.org/records/3384092/files/ground_truth_test.zip?download=1))\n",
    "\n",
    "1. Download the two mentioned files\n",
    "2. Unzip them in the Dataset/ folder. The files should be in a path like this: ./Dataset/observation_test/xxxxx.hdf5 and Dataset/ground_truth_test/xxxxx.hdf5\n",
    "\n",
    "Now, you can run the Section 1 below (it will generate ~7000 images, so it will take some time).\n",
    "\n",
    "##### Model Preparation\n",
    "\n",
    "Five pre-trained deep learning models were used in this work, along with [chaiNNer](https://chainner.app/download).\n",
    "\n",
    "1. The first model is the SRCNN model which can be download directly from [this link](https://www.dropbox.com/s/rxluu1y8ptjm4rn/srcnn_x2.pth?dl=0). If the link fails, you can go to this [this repository](https://github.com/yjn870/SRCNN-pytorch) to download it. You have to choose the \"Test; Model 9-5-5; Scale 2.\" model. \n",
    "2. The second model is the HAT model which can be download directly from [this link](https://drive.google.com/file/d/16xtMezHvckdWEuSiOxcO-dgOlsI0rEUg/view?usp=drive_link). If the link fails, you can go to this [this repository](https://github.com/chaiNNer-org/spandrel#model-architecture-support) to download it. You have to choose the \"HAT | Models\" link and download the \"HAT-L_SRx2_ImageNet-pretrain.pth\" file. \n",
    "3. The third model is the Real ESRGAN model which can be download directly from [this link](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth). If the link fails, you can go to this [this repository](https://github.com/chaiNNer-org/spandrel#model-architecture-support) to download it. You have to choose the \"Real-ESRGAN Compact (SRVGGNet) | Models\" link and download the \"RealESRGAN_x2plus.pth\" file. \n",
    "4. The fourth model is the SwinIR model which can be download directly from [this link](https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/001_classicalSR_DIV2K_s48w8_SwinIR-M_x2.pth). If the link fails, you can go to this [this repository](https://github.com/chaiNNer-org/spandrel#model-architecture-support) to download it. You have to choose the \"SwinIR | Models\" link and download the \"001_classicalSR_DIV2K_s48w8_SwinIR-M_x2.pth\" file. \n",
    "5. The fifth model is the DAT model which can be download directly from [this link](https://drive.google.com/file/d/1AYfLMnIqSlOJyOGabaRI48TEJh440fsN/view?usp=drive_link). If the link fails, you can go to this [this repository](https://github.com/chaiNNer-org/spandrel#model-architecture-support) to download it. You have to choose the \"DAT | Models\" link and download the \"DAT_x2.pth\" file. \n",
    "\n",
    "Download them and put in the Models/ folder!\n",
    "\n",
    "You can now run Section 2 below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - HDF to BMP and Sinogram to human-readable CT\n",
    "\n",
    "For this cornerstone project, only ground_truth_test.zip and observation_test.zip are needed. After unzipping, set your \"root_path\" variable based on your own desired folders' structure\n",
    "\n",
    "This script processes two datasets (ground truth and observation) from HDF5 files, typically used in medical imaging or computed tomography reconstruction scenarios. It converts these data into BMP image format and saves them into pre-defined directories.\n",
    "\n",
    "Key steps in the code:\n",
    "\n",
    "Directory Creation: The script ensures that the directories for storing the images (both ground truth and observation) exist. If they don't, it creates them using the create_dir function.\n",
    "\n",
    "Ground Truth and Observation Processing: It reads HDF5 files containing arrays of image data. The ground truth data is directly saved as images after rotating them. The observation data, on the other hand, undergoes more complex processing: it performs a reconstruction using the iradon function (Filtered Back Projection) and crops the resulting image to a standard size before saving.\n",
    "\n",
    "Error Handling: The code handles potential errors in reading files. If an error occurs while processing observation data, the corresponding ground truth image is deleted to maintain data consistency.\n",
    "\n",
    "FBP Reconstruction: The observation data is reconstructed using the iradon function from the skimage.transform library. This method applies a transformation called Filtered Back Projection, commonly used in medical imaging to reconstruct cross-sectional images from projection data (as in CT scans).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script processes medical imaging data stored in HDF5 format. Specifically, it reads the\n",
    "ground truth and observation test datasets, processes them into images, and saves them in BMP format.\n",
    "\n",
    "The script does the following:\n",
    "1. Creates directories for saving ground truth and observation images if they don't exist.\n",
    "2. Reads ground truth and observation HDF5 files.\n",
    "3. For each file:\n",
    "   - Ground truth images are rotated and saved directly.\n",
    "   - Observation data is used to reconstruct images using the Filtered Back Projection (FBP) technique,\n",
    "     crop the result, and save the reconstructed images.\n",
    "4. If an error occurs in reading the observation file, the corresponding ground truth image is deleted to\n",
    "   ensure data consistency.\n",
    "\n",
    "Dependencies:\n",
    "- h5py: For reading HDF5 files.\n",
    "- numpy: For numerical operations, including image manipulation.\n",
    "- matplotlib: For saving images in BMP format.\n",
    "- skimage: For performing the Filtered Back Projection (FBP) reconstruction.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import iradon\n",
    "\n",
    "# Path to the dataset folder containing HDF5 files\n",
    "root_path = \"./\"\n",
    "dataset_path = root_path+ 'Dataset/'\n",
    "\n",
    "# List of folder names within the dataset containing various datasets (train, test, validation)\n",
    "hdf5_folders = [\n",
    "    'ground_truth_train',\n",
    "    'ground_truth_test',\n",
    "    'ground_truth_validation',\n",
    "    'observation_train',\n",
    "    'observation_test',\n",
    "    'observation_validation',\n",
    "]\n",
    "\n",
    "# Function to create directories if they do not exist\n",
    "\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "# Process ground truth test data by saving images in a specific directory\n",
    "ground_truth_test_images_dir = os.path.join(\n",
    "    dataset_path, 'ground_truth_test_images')\n",
    "create_dir(ground_truth_test_images_dir)\n",
    "\n",
    "# Process observation test data by saving images in a specific directory\n",
    "observation_test_images_dir = os.path.join(\n",
    "    dataset_path, 'observation_test_images')\n",
    "create_dir(observation_test_images_dir)\n",
    "\n",
    "# Get a sorted list of ground truth and observation HDF5 files\n",
    "ground_truth_files = sorted(os.listdir(\n",
    "    os.path.join(dataset_path, hdf5_folders[1])))\n",
    "observation_files = sorted(os.listdir(\n",
    "    os.path.join(dataset_path, hdf5_folders[4])))\n",
    "\n",
    "# Initialize counters for naming the saved image files\n",
    "ground_truth_counter = 0\n",
    "observation_counter = 0\n",
    "\n",
    "# Process both ground truth and observation HDF5 files\n",
    "for gt_file_name, obs_file_name in zip(ground_truth_files, observation_files):\n",
    "    gt_file_path = os.path.join(dataset_path, hdf5_folders[1], gt_file_name)\n",
    "    obs_file_path = os.path.join(dataset_path, hdf5_folders[4], obs_file_name)\n",
    "\n",
    "    # Try opening and processing the ground truth HDF5 file\n",
    "    try:\n",
    "        with h5py.File(gt_file_path, 'r') as gt_hdf5_file:\n",
    "            for j in range(len(gt_hdf5_file['data'])):\n",
    "                # Extract the ground truth image data\n",
    "                gt_data = gt_hdf5_file['data'][j]\n",
    "\n",
    "                # Save the ground truth image after rotating 90 degrees\n",
    "                gt_image_name = f'ground_truth_test_{ground_truth_counter:04d}.bmp'\n",
    "                plt.imsave(os.path.join(ground_truth_test_images_dir,\n",
    "                           gt_image_name), arr=np.rot90(gt_data, k=1), cmap='gray')\n",
    "                ground_truth_counter += 1\n",
    "\n",
    "        # Try opening and processing the corresponding observation HDF5 file\n",
    "        try:\n",
    "            with h5py.File(obs_file_path, 'r') as obs_hdf5_file:\n",
    "                for j in range(len(obs_hdf5_file['data'])):\n",
    "                    # Extract observation data and transpose for FBP reconstruction\n",
    "                    obs_data = obs_hdf5_file['data'][j]\n",
    "                    obs_data = np.transpose(obs_data)\n",
    "\n",
    "                    # Generate angles for FBP reconstruction\n",
    "                    theta = np.linspace(0., 180., max(\n",
    "                        obs_data.shape), endpoint=False)\n",
    "\n",
    "                    # Perform FBP (Filtered Back Projection) reconstruction\n",
    "                    reconstruction_fbp = iradon(\n",
    "                        obs_data, theta=theta, filter_name='ramp')\n",
    "\n",
    "                    # Crop the reconstructed image to a fixed size (362x362)\n",
    "                    crop_height = min(362, reconstruction_fbp.shape[0])\n",
    "                    crop_width = min(362, reconstruction_fbp.shape[1])\n",
    "                    start_row = (\n",
    "                        reconstruction_fbp.shape[0] - crop_height) // 2\n",
    "                    start_col = (reconstruction_fbp.shape[1] - crop_width) // 2\n",
    "                    reconstruction_fbp = reconstruction_fbp[start_row:start_row +\n",
    "                                                            crop_height, start_col:start_col + crop_width]\n",
    "\n",
    "                    # Save the observation image\n",
    "                    obs_image_name = f'observation_test_{observation_counter:04d}.bmp'\n",
    "                    plt.imsave(os.path.join(observation_test_images_dir,\n",
    "                               obs_image_name), arr=reconstruction_fbp, cmap='gray')\n",
    "                    observation_counter += 1\n",
    "\n",
    "        except OSError as e:\n",
    "            # Error handling for observation files\n",
    "            print(f\"Error opening {obs_file_path}: {e}\")\n",
    "            # Remove the corresponding ground truth image if observation fails\n",
    "            os.remove(os.path.join(ground_truth_test_images_dir, gt_image_name))\n",
    "            ground_truth_counter -= 1  # Adjust the counter to maintain consistency\n",
    "\n",
    "    except OSError as e:\n",
    "        # Error handling for ground truth files\n",
    "        print(f\"Error opening {gt_file_path}: {e}\")\n",
    "        # Remove the corresponding observation file if ground truth processing fails\n",
    "        if os.path.exists(obs_file_path):\n",
    "            os.remove(obs_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - SRCNN as an ONNX framework\n",
    "\n",
    "This code defines a Super-Resolution Convolutional Neural Network (SRCNN) designed for single-channel (grayscale) images. The SRCNN model is a simple but effective deep learning model that performs image super-resolution by mapping low-resolution images to high-resolution counterparts through three convolutional layers. This script loads pre-trained weights for the SRCNN, sets the model to evaluation mode, and exports it to the ONNX format.\n",
    "\n",
    "Key components of the code:\n",
    "\n",
    "Model Definition: The SRCNN class defines a neural network with three convolutional layers. Each layer applies a different level of feature extraction and image refinement.\n",
    "\n",
    "The first layer uses 64 filters to extract features.\n",
    "The second layer reduces the feature space to 32 dimensions.\n",
    "The third layer outputs the reconstructed high-resolution image.\n",
    "ReLU activation is applied after the first two convolutional layers to introduce non-linearity.\n",
    "Loading Pre-Trained Weights: The script loads pre-trained weights (from a .pth file) into the SRCNN model. These weights are assumed to have been trained on an external dataset.\n",
    "\n",
    "Evaluation Mode: Once the model is loaded with weights, it's set to evaluation mode. This ensures that certain layers (like dropout or batch normalization, if they existed) behave properly during inference.\n",
    "\n",
    "Export to ONNX: The model is exported to the ONNX format using the torch.onnx.export function, with a dummy input tensor that simulates a grayscale (single-channel) image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script defines and exports a Super-Resolution Convolutional Neural Network (SRCNN) model to the ONNX format.\n",
    "SRCNN is designed for single-channel (grayscale) image input, and this implementation is based on the architecture\n",
    "outlined in the SRCNN paper for image super-resolution.\n",
    "\n",
    "The script includes the following steps:\n",
    "1. Define the SRCNN model architecture.\n",
    "2. Load pre-trained weights (state_dict) into the model.\n",
    "3. Set the model to evaluation mode.\n",
    "4. Export the model to ONNX format using a dummy single-channel image input for format specification.\n",
    "\n",
    "Dependencies:\n",
    "- torch: For defining and loading the SRCNN model and its weights.\n",
    "- torch.onnx: For exporting the PyTorch model to ONNX format.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.onnx\n",
    "\n",
    "# Define the SRCNN model class for single-channel input images\n",
    "\n",
    "\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        \"\"\"\n",
    "        Initializes the SRCNN model.\n",
    "\n",
    "        Args:\n",
    "            num_channels (int): Number of input channels (default is 1 for grayscale images).\n",
    "        \"\"\"\n",
    "        super(SRCNN, self).__init__()\n",
    "        # First convolutional layer with 64 filters, kernel size 9x9, and padding to keep output size same as input\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n",
    "\n",
    "        # Second convolutional layer with 32 filters, kernel size 5x5, and padding to maintain spatial dimensions\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n",
    "\n",
    "        # Third convolutional layer that outputs the final single-channel (grayscale) image\n",
    "        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 // 2)\n",
    "\n",
    "        # ReLU activation function applied after the first two convolutional layers\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor of shape [batch_size, num_channels, height, width].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output high-resolution image tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        # Pass input through the first convolutional layer followed by ReLU activation\n",
    "        x = self.relu(self.conv1(x))\n",
    "\n",
    "        # Pass through the second convolutional layer followed by ReLU activation\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        # Pass through the third convolutional layer to generate the final output image\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the SRCNN model for single-channel (grayscale) input\n",
    "model = SRCNN(num_channels=1)\n",
    "\n",
    "# Load the pre-trained model weights from the .pth file\n",
    "# Adjust the path to where the pre-trained weights are located\n",
    "state_dict = torch.load('Models/srcnn_x2.pth')\n",
    "model.load_state_dict(state_dict)  # Load the weights into the model\n",
    "\n",
    "# Set the model to evaluation mode to ensure correct behavior during inference (e.g., disabling dropout if present)\n",
    "model.eval()\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model,                      # The model to be exported\n",
    "    # Dummy input of size [batch_size, num_channels, height, width] for grayscale images\n",
    "    torch.randn(1, 1, 224, 224),\n",
    "    \"Models/srcnn_x2.onnx\",    # Path where the ONNX model will be saved\n",
    "    export_params=True,          # Whether to store the trained weights inside the ONNX model\n",
    "    verbose=False,               # Whether to print detailed logs during export\n",
    "    opset_version=10             # ONNX opset version to ensure compatibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - chaiNNEr\n",
    "\n",
    "First, we have to edit the chaiNNEr template file to run it. Edit the \"edit_chaiNNer_file.py\" file, settings the three following variables properly:\n",
    "\n",
    "- dataset_folder\n",
    "- images_folder\n",
    "- models_folder\n",
    "\n",
    "Define them using the full path to the Dataset/, Models/ and Images/ folder. Examples considering Linux and Windows were provided.\n",
    "\n",
    "Run the \"edit_chaiNNer_file.py\" file to generate the chaiNNer file (\"CT Super Resolution_edited.chn\") with the correct folders.\n",
    "\n",
    "Open [chaiNNer](ttps://chainner.app/download) (download and extract/install it if you haven't already). \n",
    "\n",
    "Load the \"CT Super Resolution_edited.chn\" file inside chaiNNer. Install dependencies if chaiNNer asks to do it. \n",
    "\n",
    "Since we edited the file outside chaiNNer, using the \"edit_chaiNNer_file.py\" script, it will raise a warning. You can ignore it.\n",
    "\n",
    "If everything is ready, you can click in the green \"play\" button on top of chaiNNer. It will process ~3500 images using 5 deep learning methods, so it will take some time. \n",
    "\n",
    "Then, you can go to the Part 2 of this repository - Image Evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
